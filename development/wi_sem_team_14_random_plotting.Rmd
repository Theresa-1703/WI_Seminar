---
title: "wi_sem_team_14_plotting"
author: "Chris"
date: "7/11/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
# using pacman to laod and install any packages
install.packages("pacman")

# load packages
pacman::p_load(pacman, tidyverse, rio, dplyr, ggplot2, plotly, tidyr, rmarkdown, tinytex, lubridate, gridExtra)

knitr::opts_chunk$set(echo = TRUE)
```


```{r dataloading, include=FALSE}
# read dataset
data_event_log <- read_delim(
  "../data/event_log.csv",
  delim=';'
)
```

Wertebereich fÃ¼r interessante Spalten ausgeben
```{r uniques}
unique(data_event_log$ACTIVITY)
unique(data_event_log$DEVICETYPE)
unique(data_event_log$SERVICEPOINT)
unique(data_event_log$REPAIR_IN_TIME_5D)
```

## Data cleaning

```{r data_cleaning, echo=FALSE}
# cleaning the data
# exclude rows containing null-values
clean_logs <- na.omit(data_event_log)
corrupted_logs <- subset(data_event_log,
                          is.na(DEVICETYPE) | 
                          is.na(SERVICEPOINT)
                         )
df_clean_logs <- as.data.frame(do.call(cbind, clean_logs))

df_clean_logs$TIMESTAMP <- as.POSIXct(clean_logs$TIMESTAMP, tz="",format="%Y-%m-%d %H:%M:%OS", optional = FALSE)
```

Some data exploration

```{r data_exploration, echo=FALSE}
print('Number of datapoint in the clean dataset:')
nrow(df_clean_logs)
print("Number of unique case IDs:")
unique(clean_logs$CASE_ID) %>% length()
```
### Some data modification and testing

* creating column DATE (timestamps without the time information)
* creating column WEEKDAY (not sure if we need this as a column in the dataset, can just compute it insitro when needed)

```{r data_modification, echo=FALSE}
df_cl_mod <- df_clean_logs

# creating DATE column (calculating for each row)
df_cl_mod$DATE <- with(df_clean_logs, as.Date(TIMESTAMP))

# creating WEEKDAY column
df_cl_mod$WEEKDAY <- weekdays(df_cl_mod$DATE)


sleeper_cases <- c("Case5304", "Case5502", "Case5544")

df_sleeper_cases <- df_clean_logs[df_cl_mod$CASE_ID%in%sleeper_cases,]
df_cl_mod <- df_cl_mod[!(df_clean_logs$CASE_ID%in%sleeper_cases),]

# checking if some case ids have datapoints with mixed rit values (rit = repair in time 5 days)
df_check_for_corrupt_repair <- df_cl_mod %>% group_by(CASE_ID) %>% summarise(ACTIVITY_COUNT = n(),
                                                                             RIT = first(REPAIR_IN_TIME_5D),
                                                                             RIT_GROUP_COUNTER = (table(REPAIR_IN_TIME_5D))[2],
                                                                             ERROR = !((RIT == 0 & 0 == RIT_GROUP_COUNTER) | (RIT ==1 & RIT_GROUP_COUNTER == ACTIVITY_COUNT))
                                                                             )
```

#### creating a new dataframe containing aggregated information per case_id

```{r create_new_dataframe_case_id_aggregats}
case_id_aggregated_information <- 
  df_cl_mod %>% group_by(CASE_ID) %>% 
  summarise(SERVICEPOINT = first(SERVICEPOINT),
            DEVICETYPE = first(DEVICETYPE), 
            ACTIVITY_COUNT = n(),
            START_DATETIME = min(TIMESTAMP), 
            END_DATETIME = max(TIMESTAMP),
            RIT = first(REPAIR_IN_TIME_5D), 
            THROUGHPUT_TIME_HOURS = 
              as.numeric(difftime(END_DATETIME, START_DATETIME, units="hours")),
            START_MONTH = month(START_DATETIME),
            START_YEAR = year(START_DATETIME),
)
# order dataset based on START_DATETIME
case_id_aggregated_information <- case_id_aggregated_information[order(case_id_aggregated_information$START_DATETIME, decreasing = FALSE),]
            
rit_cases_too_long <- case_id_aggregated_information[case_id_aggregated_information$RIT == 1 & case_id_aggregated_information$THROUGHPUT_TIME_HOURS>120,]
```

### new dataframe containing duration information of each activity log

```{}
activity_information <- 
  df_cl_mod %>% 
  group_by(CASE_ID, ACTIVITY) %>%
  summarise(
    START_TS = min(TIMESTAMP)
  )

calc_duration = function(x){
  current_activity <- x[2]
  current_case_id <- x[1]
  current_activity_ts <- as.POSIXct(x[3], tz="UTC",format="%Y-%m-%d %H:%M:%OS", optional = FALSE)
  
  # print(paste("current case id: ", current_case_id))
  # print(paste("current activity: " , current_activity))
  # print(paste("current activity ts: " , current_activity_ts))
  
  ts_df <- as.data.frame(select(df_cl_mod[df_cl_mod$CASE_ID == current_case_id & df_cl_mod$ACTIVITY != current_activity, ], ACTIVITY, TIMESTAMP))
  # renaming
  names(ts_df)[names(ts_df)=="ACTIVITY"] <- "NEXT_ACTIVITY"
  names(ts_df)[names(ts_df)=="TIMESTAMP"] <- "NEXT_ACTIVITY_TS"
  
  # print(ts_df$TIMESTAMP)
  
  ts_df <- ts_df[(difftime(ts_df$NEXT_ACTIVITY_TS, current_activity_ts)>0), ]
  
  ts_df[order(ts_df$NEXT_ACTIVITY_TS), ]
  
  # print(ts_df$TIMESTAMP)
  # print(ts_df$ACTIVITY)
  
  re <- ts_df[1:1, ]
  re$ACTIVITY <- current_activity
  re$CASE_ID <- current_case_id
  
  return(re)
}

re_list <- apply(activity_information, 1 , calc_duration)
print("apply done")
re_df <- as.data.frame(do.call(rbind, re_list)) 

merge_columns <- c("CASE_ID", "ACTIVITY")
activity_information <- merge(activity_information, re_df, by=merge_columns, all.x = TRUE)

activity_information$DURATION = difftime(activity_information$NEXT_ACTIVITY_TS, activity_information$START_TS, units = "hours")

```

### new dataframe containg throughput summaries per month

```{r}
# grp by month
tp_time_monthly_avg <- 
  case_id_aggregated_information %>% 
  group_by(START_MONTH, START_YEAR) %>% 
  summarise(
    TP_TIME_MEAN = mean(THROUGHPUT_TIME_HOURS),
    TP_TIME_MEDIAN = median(THROUGHPUT_TIME_HOURS),
    CASE_COUNT = n()
  )
```

write out the new datasets to csv
```{r save_modifed_data}
# writing the modified df to csv with relative path to the folder "data"
# write.csv(df_cl_mod, "../data/modified_logs.csv")
# write.csv(case_id_aggregated_information, "../data/case_id_aggregated_information.csv")
# write.csv(activity_information, "../data/activity_information.csv")
```


## Basic univariate plotting

our quantitative variables are :

* NONE ??

our qualitative variables are:

* CASE_ID (not sure tbh, bc this is part of the "primary key" of the dataset entities) {string} <- maybe convert to integer for easier processing
* ACTIVITY {string}
* SERVICEPOINT {char}
* DEVICETYPE {string}
* REPAIR_IN_TIME {double} <- maybe convert to boolean for easier processing

neither?:

* TIMESTAMP {double - feels more like a string tho}

### Plotting the frequency of our qualitative variables:
```{r univariate_plotting, echo=FALSE}
# activity count
activity_count <- table(clean_logs$ACTIVITY)
barplot(main="logs per activitytype", activity_count)

# rep in time
timely_repair <- table(clean_logs$REPAIR_IN_TIME_5D)
barplot(main="repair in time", timely_repair)

# logs per devicetype
devicetypes_counter <- table(clean_logs$DEVICETYPE)
barplot(main="logs per devicetype", devicetypes_counter)

# servicepoints
servicepoints <- table(clean_logs$SERVICEPOINT)
barplot(main="logs per servicepoint",servicepoints)

# logs weekday
logs_per_weekday <- table(df_cl_mod$WEEKDAY)
barplot(main="logs per weekday", logs_per_weekday)

# logs per date
logs_per_date <- table(df_cl_mod$DATE)
plot(main="logs per date", logs_per_date)

```
```{r basic plotting the case_id_aggregated_information dataset}
plot(main="throughput time ordered by START_DATETIME",as.Date(case_id_aggregated_information$START_DATETIME), case_id_aggregated_information$THROUGHPUT_TIME_HOURS, type = 'h')

```

```{r fig.align="center", echo = FALSE,fig.width = 14}
 ggplot(case_id_aggregated_information, aes(x = START_DATETIME, y = THROUGHPUT_TIME_HOURS)) + 
  geom_point(color = "darkorchid4", size=.2) +
  labs(title = "Throughput time (in hrs) per case sorted by start date",
           y = "Throughput time in hours",
           x = "Start datetime")
```
```{r tp-time per month}


ggplot(case_id_aggregated_information, aes(x = START_DATETIME, y = THROUGHPUT_TIME_HOURS)) + 
  geom_point(color = "darkorchid4", size=.2) +
  labs(title = "Throughput time (in hrs) per case sorted by start date",
           y = "Throughput time in hours",
           x = "Start datetime")

```

```{r fig.align="center", echo = FALSE,fig.width = 14}
gglines_tptime_rit_cases <- 
  ggplot(case_id_aggregated_information[case_id_aggregated_information$RIT == 1 ,], aes(x = START_DATETIME, y = THROUGHPUT_TIME_HOURS)) + 
  geom_point(color = "darkorchid4", size=.2) +
  labs(title = "Throughput time (in hrs) of RIT cases sorted by start datetime",
           y = "Throughput time in hours",
           x = "Start datetime")

gglines_tptime_rit_cases_hours_overflow <- 
  ggplot(case_id_aggregated_information[case_id_aggregated_information$RIT == 1 & case_id_aggregated_information$THROUGHPUT_TIME_HOURS>120,], aes(x = START_DATETIME, y = THROUGHPUT_TIME_HOURS))+
  geom_point(color = "darkorchid4", size=.2) +
  labs(title = "Throughput time (in hrs) of RIT cases over 120hrs",
           y = "Throughput time in hours",
           x = "Start datetime")

grid.arrange(gglines_tptime_rit_cases, gglines_tptime_rit_cases_hours_overflow, ncol = 2)
```
```{r ig.align="center", echo = FALSE,fig.width = 14}

```

```{r random_plotting, echo=FALSE}
# plot the data using ggplot2 and pipes

df_date_sum_case_id <- df_cl_mod %>% group_by(DATE) %>% tally()

df_date_sum_case_id %>%
ggplot(aes(x = DATE, y = n)) +
      geom_point(color = "darkorchid4") +
      labs(title = "Cases per date",
           y = "Num of cases)",
           x = "Date") + theme_bw(base_size = 15)
```













